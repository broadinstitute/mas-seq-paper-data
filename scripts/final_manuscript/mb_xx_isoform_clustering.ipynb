{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brave-brunswick",
   "metadata": {},
   "source": [
    "## Isoform analysis by genome alignment, junction-based alignment vectorization, and decision-tree isoform assignment\n",
    "\n",
    "This notebook provides the ability for both manual isoform annotation (by means of specifying a decision tree) and automated clustering (using DBSCAN).\n",
    "\n",
    "**Inputs and Outputs**\n",
    "- Inputs:\n",
    "  - transcriptome annotation `.gtf` file (e.g. GENCODE v37)\n",
    "  - genome assembly reference `.fasta` file\n",
    "  - MAS-seq array elements `.bam` file\n",
    "  - A gene name to study (e.g. PTPRC)\n",
    "- Outputs:\n",
    "  - Figures\n",
    "  - `.bam` file for all the queries mapping to the selected gene\n",
    "  - `.bam` file for all the queries mapping to the selected gene stratified by leiden cluster id\n",
    "  - `.h5ad` AnnData file of manual isoform annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./sources'))\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import colorcet as cc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import logging\n",
    "import pickle\n",
    "import gffutils\n",
    "import pysam\n",
    "import umap\n",
    "import scanpy as sc\n",
    "import pickle\n",
    "\n",
    "import isoform_utils as iso\n",
    "import vis_utils as vis\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "log_info = print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-wrong",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = '/home/jupyter/mb-ml-data-disk/MAS-seq-analysis'\n",
    "prefix = 'M132TS_MAS_15x.revised_v2'\n",
    "\n",
    "gtf_path = os.path.join(\n",
    "    repo_root, 'resources/gencode.v37.annotation.gtf')\n",
    "db_path = os.path.join(\n",
    "    repo_root, 'resources/gencode.v37.annotation.db')\n",
    "gs_to_gid_map_pkl_path = os.path.join(\n",
    "    repo_root, 'resources/gencode.v37.annotation.gs_to_gid_map.pkl')\n",
    "ref_path = os.path.join(\n",
    "    repo_root, 'resources/Homo_sapiens.GRCh38.dna.primary_assembly.fa')\n",
    "\n",
    "# path to the MAS-seq .bam file\n",
    "bam_path = os.path.join(\n",
    "    repo_root,\n",
    "    'data/t-cell-vdj/long/alignments/revised_v2/M132TS_MAS_15x_annotated_array_elements_for_quant_with_gene_names.corrected_umis.bam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-sally",
   "metadata": {},
   "source": [
    "## (Optional) Make a gffutils `.db` file\n",
    "\n",
    "Must be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(db_path):\n",
    "    \n",
    "    # this takes a while ...\n",
    "    fn = gffutils.example_filename(gtf_path)\n",
    "    db = gffutils.create_db(\n",
    "        fn,\n",
    "        db_path,\n",
    "        keep_order=True,\n",
    "        disable_infer_genes=True,\n",
    "        disable_infer_transcripts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-culture",
   "metadata": {},
   "source": [
    "## (Optional) Gene symbol to Gene ID map\n",
    "\n",
    "Must be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(gs_to_gid_map_pkl_path):\n",
    "    \n",
    "    # this takes a while ...\n",
    "    gs_to_gid_map = dict()\n",
    "    for gene in db.all_features():\n",
    "        gid = gene.attributes['gene_id'][0]\n",
    "        gs = gene.attributes['gene_name'][0]\n",
    "        gs_to_gid_map[gs] = gid\n",
    "\n",
    "    with open(gs_to_gid_map_pkl_path, 'wb') as f:\n",
    "        pickle.dump(gs_to_gid_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-laser",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene symbol to study\n",
    "gs = 'PTPRC'\n",
    "\n",
    "# smallest vectorization interval length\n",
    "min_vectorization_interval_length = 5\n",
    "\n",
    "# start and end padding for fetching alignments\n",
    "fetch_padding = 250  # fetch records with this much paiing\n",
    "gene_body_padding = 500  # ultimately, allow alignments within this much margin\n",
    "\n",
    "# minimum / maximum alignment mapping quality \n",
    "min_mapping_quality = 1\n",
    "max_mapping_quality = np.inf\n",
    "\n",
    "# options: 'gtf', 'alignments'\n",
    "junctions_source = 'alignments'\n",
    "\n",
    "# what .gtf features to include for vectorization? (only used if junctions_source == 'gtf')\n",
    "included_feature_types = ['UTR', 'exon', 'intron']\n",
    "\n",
    "# options for alignment-based junction inference\n",
    "alignment_block_min_gap = 5\n",
    "junction_merge_gap = 3\n",
    "min_rel_junction_read_support = 0.01\n",
    "min_abs_junction_read_support = 100\n",
    "include_end_segments = False\n",
    "consider_read_endpoints_as_junctions = True\n",
    "\n",
    "# options: 'relative', 'absolute', 'quantized' \n",
    "vectorization_strategy = 'relative'\n",
    "\n",
    "# minimum vectorization interval overlap to quantize to 1 (only used if vectorization_strategy == 'quantized')\n",
    "rel_overlap_quantization_threshold = 0.25\n",
    "abs_overlap_quantization_threshold = 100\n",
    "\n",
    "# remove records that overlap with too few of the vectorization intervals\n",
    "min_vector_weight = 1.\n",
    "\n",
    "# (for plotting) remove segments with coverage less than `min_covearge_for_compressed_plot`\n",
    "min_covearge_for_compressed_plot = 5\n",
    "\n",
    "# (for plotting) pad kept genomics regions from left and right by `nonzero_coverage_positions_pad`\n",
    "nonzero_coverage_positions_pad = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-australian",
   "metadata": {},
   "source": [
    "## Load GENCODE annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gffutils .db\n",
    "db = gffutils.FeatureDB(db_path)\n",
    "\n",
    "# load gs to gid map\n",
    "with open(gs_to_gid_map_pkl_path, 'rb') as f:\n",
    "    gs_to_gid_map = pickle.Unpickler(f).load()\n",
    "\n",
    "gid = gs_to_gid_map[gs.upper()]\n",
    "\n",
    "# generate introns\n",
    "gene_db = gffutils.create_db(\n",
    "    db.children(gid),\n",
    "    ':memory:',\n",
    "    keep_order=True,\n",
    "    disable_infer_genes=True,\n",
    "    disable_infer_transcripts=True)\n",
    "\n",
    "gene_db.update(gene_db.create_introns())\n",
    "\n",
    "# get feature intervals\n",
    "feature_interval_list = iso.get_feature_interval_list(gene_db, included_feature_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-genre",
   "metadata": {},
   "source": [
    "## Load BAM alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_start = min(interval.start for interval in feature_interval_list) - fetch_padding\n",
    "fetch_end = max(interval.end for interval in feature_interval_list) + fetch_padding\n",
    "contig = feature_interval_list[0].contig\n",
    "contig_without_str = contig if contig.find('chr') == -1 else contig[3:]\n",
    "\n",
    "# load alignments\n",
    "raw_alignment_list, alignment_stats = iso.fetch_alignments(\n",
    "    bam_path=bam_path,\n",
    "    fetch_contig=contig,\n",
    "    fetch_start=fetch_start,\n",
    "    fetch_end=fetch_end,\n",
    "    gene_body_padding=gene_body_padding,\n",
    "    min_mapping_quality=min_mapping_quality,\n",
    "    max_mapping_quality=max_mapping_quality)\n",
    "\n",
    "# update the fetch region based on the endpoints of the alignments\n",
    "fetch_start = np.min([fetch_start] + [aln.reference_start for aln in raw_alignment_list])\n",
    "fetch_end = np.max([fetch_end] + [aln.reference_end for aln in raw_alignment_list])\n",
    "\n",
    "# load reference\n",
    "ref_file = pysam.Fastafile(ref_path)\n",
    "gene_seq = ref_file.fetch(start=fetch_start, end=fetch_end, reference=contig_without_str)\n",
    "\n",
    "# statistics\n",
    "log_info(f'Fetch reference interval: {contig, fetch_start, fetch_end}')\n",
    "log_info(f'Total queries: {alignment_stats[\"num_total_queries\"]}')\n",
    "log_info(f'Total alignments: {alignment_stats[\"num_total_alignments\"]}')\n",
    "log_info(f'Number of queries with single low quality alignments: {alignment_stats[\"num_queries_with_single_low_quality_alignment\"]}')\n",
    "log_info(f'Number of queries with single high quality alignments: {alignment_stats[\"num_queries_with_single_high_quality_alignment\"]}')\n",
    "log_info(f'Number of queries with multiple low quality alignments: {alignment_stats[\"num_queries_with_multiple_low_quality_alignments\"]}')\n",
    "log_info(f'Number of queries with multiple high quality alignments: {alignment_stats[\"num_queries_with_multiple_high_quality_alignments\"]}')\n",
    "log_info(f'Number of multi-mapped queries with a single outstanding alignment: {alignment_stats[\"num_queries_with_outstanding_alignment\"]}')\n",
    "log_info(f'Number of queries with out out gene body alignment: {alignment_stats[\"num_queries_with_out_of_gene_body_alignments\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-windsor",
   "metadata": {},
   "source": [
    "## Generate vectorization intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "if junctions_source == 'gtf':\n",
    "    \n",
    "    vectorization_interval_list = iso.get_vectorization_interval_list_from_features(feature_interval_list)\n",
    "    \n",
    "elif junctions_source == 'alignments':\n",
    "    \n",
    "    raw_junctions, raw_counts = iso.get_junctions_and_counts_from_alignments(\n",
    "        raw_alignment_list,\n",
    "        alignment_block_min_gap=alignment_block_min_gap,\n",
    "        consider_read_endpoints_as_junctions=consider_read_endpoints_as_junctions)\n",
    "\n",
    "    merged_junctions, merged_counts = iso.merge_junctions(\n",
    "        raw_junctions,\n",
    "        raw_counts,\n",
    "        junction_merge_gap=junction_merge_gap)\n",
    "\n",
    "    filtered_junctions, filtered_counts = iso.filter_junctions(\n",
    "        merged_junctions,\n",
    "        merged_counts,\n",
    "        total_reads=len(raw_alignment_list),\n",
    "        min_rel_junction_read_support=min_rel_junction_read_support,\n",
    "        min_abs_junction_read_support=min_abs_junction_read_support)\n",
    "\n",
    "    reference_start, reference_end = iso.get_alignment_bounds(raw_alignment_list)\n",
    "\n",
    "    vectorization_interval_list = iso.get_vectorization_interval_list_from_junctions(\n",
    "        junctions=filtered_junctions,\n",
    "        reference_start=reference_start,\n",
    "        reference_end=reference_end,\n",
    "        reference_contig=contig,\n",
    "        gene_id=gid,\n",
    "        include_end_segments=include_end_segments,\n",
    "        min_vectorization_interval_length=min_vectorization_interval_length)\n",
    "    \n",
    "    log_info(f'Number of raw junctions: {len(raw_junctions)}')\n",
    "    log_info(f'Number of merged junctions: {len(merged_junctions)}')\n",
    "    log_info(f'Number of filtered junctions: {len(filtered_junctions)}')\n",
    "\n",
    "else:\n",
    "    \n",
    "    raise ValueError\n",
    "    \n",
    "# statistics\n",
    "num_vectorization_intervals = len(vectorization_interval_list)\n",
    "vectorization_interval_lengths = [interval.end - interval.start + 1 for interval in vectorization_interval_list]\n",
    "min_vectorization_interval_length = min(vectorization_interval_lengths)\n",
    "max_vectorization_interval_length = max(vectorization_interval_lengths)\n",
    "\n",
    "log_info(f'Number of vectorization genomics intervals: {num_vectorization_intervals}')\n",
    "log_info(f'Smallest vectorization interval length: {min_vectorization_interval_length}')\n",
    "log_info(f'Largest vectorization interval length: {max_vectorization_interval_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectorized representation\n",
    "vectorized_alignments_nv = iso.get_vectorized_alignments_matrix(\n",
    "    alignment_list=raw_alignment_list,\n",
    "    vectorization_interval_list=vectorization_interval_list,\n",
    "    abs_overlap_quantization_threshold=abs_overlap_quantization_threshold,\n",
    "    rel_overlap_quantization_threshold=rel_overlap_quantization_threshold,\n",
    "    vectorization_strategy=vectorization_strategy)\n",
    "\n",
    "# get coverage\n",
    "coverage_matrix_ni = iso.get_binary_reference_coverage_sparse_matrix(\n",
    "    alignment_list=raw_alignment_list,\n",
    "    ref_interval_start=fetch_start,\n",
    "    ref_interval_end=fetch_end)\n",
    "\n",
    "total_vector_weight_n = np.sum(vectorized_alignments_nv, -1)\n",
    "total_coverage_i = np.asarray(coverage_matrix_ni.sum(0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "n_alignments_all = len(raw_alignment_list)\n",
    "passing_alignments_n = np.ones((n_alignments_all,), dtype=np.bool)\n",
    "\n",
    "passing_alignments_n = passing_alignments_n & (total_vector_weight_n > min_vector_weight)\n",
    "n_removed_low_weight = n_alignments_all - np.sum(passing_alignments_n)\n",
    "\n",
    "filtered_alignment_list = [\n",
    "    raw_alignment_list[idx] for idx in range(len(raw_alignment_list)) if passing_alignments_n[idx]]\n",
    "filtered_vectorized_alignments_mv = vectorized_alignments_nv[passing_alignments_n]\n",
    "filtered_coverage_matrix_mi = coverage_matrix_ni[passing_alignments_n]\n",
    "\n",
    "log_info(f'Removed {n_removed_low_weight} alignments for having relative segment coverage weight <= {min_vector_weight}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-officer",
   "metadata": {},
   "source": [
    "## Visualize vectorization intervals, junctions, coverage, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have ipywidgets installed, you can make interactive plots by replacing\n",
    "# the following line with:\n",
    "#\n",
    "# %matplotlib widget\n",
    "#\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "coverage_transform = lambda x: x\n",
    "\n",
    "# visualize intervals\n",
    "fig = plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "for idx, interval in enumerate(feature_interval_list):\n",
    "    start = interval.start\n",
    "    end = interval.end\n",
    "    ax.plot([start, end], [idx, idx], lw=1)\n",
    "ax.set_xlabel('position')\n",
    "ax.set_ylabel('interval index')\n",
    "ax.set_title('Feature Intervals (from GTF)')\n",
    "ax.set_xlim((fetch_start, fetch_end))\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "for idx, interval in enumerate(vectorization_interval_list):\n",
    "    start = interval.start\n",
    "    end = interval.end\n",
    "    ax.plot([start, end], [idx, idx], lw=1)\n",
    "ax.set_xlabel('position')\n",
    "ax.set_ylabel('interval index')\n",
    "ax.set_title('Vectorization Intervals')\n",
    "ax.set_xlim((fetch_start, fetch_end))\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "transformed_total_coverage_i = coverage_transform(total_coverage_i)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.fill_between(np.arange(fetch_start, fetch_end + 1), transformed_total_coverage_i)\n",
    "for junction in raw_junctions:\n",
    "    ax.axvline(junction, color='red', lw=0.5)\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_xlabel(f'Genomic position on {contig}')\n",
    "ax.set_xlim((fetch_start, fetch_end))\n",
    "ax.set_title('Raw junctions')\n",
    "ax.set_ylim(bottom=0.)\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.fill_between(np.arange(fetch_start, fetch_end + 1), transformed_total_coverage_i)\n",
    "for junction in merged_junctions:\n",
    "    ax.axvline(junction, color='red', lw=0.5)\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_xlabel(f'Genomic position on {contig}')\n",
    "ax.set_xlim((fetch_start, fetch_end))\n",
    "ax.set_title('Merged junctions')\n",
    "ax.set_ylim(bottom=0.)\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.fill_between(np.arange(fetch_start, fetch_end + 1), transformed_total_coverage_i)\n",
    "for junction in filtered_junctions:\n",
    "    ax.axvline(junction, color='red', lw=0.5)\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_xlabel(f'Genomic position on {contig}')\n",
    "ax.set_xlim((fetch_start, fetch_end))\n",
    "ax.set_title('Filtered junctions')\n",
    "ax.set_ylim(bottom=0.)\n",
    "\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 3))\n",
    "ax = plt.gca()\n",
    "\n",
    "nucleotide_to_color_map = {\n",
    "    'T': [1., 1., 0.],\n",
    "    'A': [0., 0., 1.],\n",
    "    'C': [1., 0., 0.],\n",
    "    'G': [0., 1., 0.]\n",
    "}\n",
    "gene_seq_im = np.asarray(list(map(nucleotide_to_color_map.get, gene_seq)))[None, :, :]\n",
    "ax.imshow(gene_seq_im, aspect='auto', extent=[fetch_start, fetch_end, 0., np.max(transformed_total_coverage_i)])\n",
    "ax.fill_between(np.arange(fetch_start, fetch_end + 1), transformed_total_coverage_i, color='white', alpha=0.7)\n",
    "ax.set_xlabel(f'Genomic position on {contig}')\n",
    "ax.set_title('Gene sequence')\n",
    "ax.set_ylim(bottom=0.)\n",
    "\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-project",
   "metadata": {},
   "source": [
    "## Detailed coverage visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark positions with above-threshold coverage for plotting\n",
    "filtered_total_coverage_i = np.asarray(filtered_coverage_matrix_mi.sum(0)).flatten()\n",
    "plot_positions_set = set()\n",
    "for pos in np.nonzero(filtered_total_coverage_i >= min_covearge_for_compressed_plot)[0].tolist() + filtered_junctions:\n",
    "    plot_positions_set.update(set(\n",
    "        np.arange(pos - nonzero_coverage_positions_pad, pos + nonzero_coverage_positions_pad, + 1).tolist()))\n",
    "plot_positions = sorted(set(pos for pos in plot_positions_set if 0 <= pos < fetch_end - fetch_start))\n",
    "\n",
    "# generate annotations for plooting\n",
    "feature_to_annotation_track_map = iso.get_feature_to_annotation_track_map(\n",
    "    feature_interval_list=feature_interval_list,\n",
    "    included_feature_types=included_feature_types,\n",
    "    ref_contig=contig,\n",
    "    ref_interval_start=fetch_start,\n",
    "    ref_interval_end=fetch_end)\n",
    "\n",
    "# add junctions to annotations\n",
    "for i_junction, junction in enumerate(filtered_junctions):\n",
    "    junction_track = np.zeros((fetch_end - fetch_start + 1), dtype=np.int8)\n",
    "    junction_track[junction - fetch_start] = 1\n",
    "    feature_to_annotation_track_map[f'junction_{i_junction}'] = junction_track\n",
    "\n",
    "# add vectorization segments to annotations\n",
    "for i_segment, segment in enumerate(vectorization_interval_list):\n",
    "    segment_track = np.zeros((fetch_end - fetch_start + 1), dtype=np.int8)\n",
    "    segment_track[(segment.start - fetch_start):(segment.end - fetch_start)] = 1\n",
    "    feature_to_annotation_track_map[f'segment_{i_segment}'] = segment_track\n",
    "    \n",
    "junction_colors = (dict(color=[0.5, 0.1, 0.1], alpha=0.5, lw=1),)\n",
    "segment_colors = (dict(color='cyan', alpha=0.1), dict(color='green', alpha=0.1))\n",
    "text_props = dict(fontsize=8, color='white', ha='center', bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "plot_annotation_manifest = []\n",
    "for i_junction in range(len(filtered_junctions)):\n",
    "    manifest = (f'junction_{i_junction}', junction_colors[i_junction % len(junction_colors)], f'{i_junction}')\n",
    "    plot_annotation_manifest.append(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# if we have too many reads, let's skip some of them uniformly (for plotting)\n",
    "max_reads_to_plot = np.inf\n",
    "n_all_reads = filtered_coverage_matrix_mi.shape[0]\n",
    "if n_all_reads > max_reads_to_plot:\n",
    "    reads_to_plot_indices = np.sort(np.random.permutation(n_all_reads)[:max_reads_to_plot])\n",
    "else:\n",
    "    reads_to_plot_indices = np.arange(n_all_reads)\n",
    "\n",
    "# coverage to plot\n",
    "coverage_img_mj = filtered_coverage_matrix_mi[reads_to_plot_indices, :][:, plot_positions].todense()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "\n",
    "# plot individual reads\n",
    "ax.imshow(coverage_img_mj, aspect='auto', cmap=plt.cm.Greys_r)\n",
    "\n",
    "# plot annotations\n",
    "text_ypos_str = 'down'\n",
    "for desc in plot_annotation_manifest:\n",
    "    name = desc[0]\n",
    "    props = desc[1]\n",
    "    text = desc[2]\n",
    "    track = feature_to_annotation_track_map[name][plot_positions]\n",
    "    nnz = np.sum(track > 0)\n",
    "    \n",
    "    if nnz < 1:\n",
    "        continue \n",
    "    elif nnz == 1:  # line\n",
    "        ax.axvline(track.argmax(), **props)\n",
    "    else:  # region\n",
    "        ax.axvspan(track.argmax(), len(track) - track[::-1].argmax(), **props)\n",
    "        \n",
    "    if len(text) > 0:\n",
    "        text_xpos = 0.5 * (len(track) - track[::-1].argmax() + track.argmax())\n",
    "        if text_ypos_str == 'down':\n",
    "            text_ypos = 0.95 * coverage_img_mj.shape[0]\n",
    "            text_ypos_str = 'up'\n",
    "        else:\n",
    "            text_ypos = 0.9 * coverage_img_mj.shape[0]\n",
    "            text_ypos_str = 'down'\n",
    "        ax.text(text_xpos, text_ypos, text, **text_props)\n",
    "        \n",
    "\n",
    "compressed_str = 'compressed coordinates; ' if min_covearge_for_compressed_plot > 0 else 'actual coordinates; '\n",
    "suffix_str = 'compressed_coordinates' if min_covearge_for_compressed_plot > 0 else 'actual_coordinates'\n",
    "ax.set_xlabel(f'Genomic position (forward strand) [{compressed_str}start offset: {contig}:{fetch_start}]')\n",
    "ax.set_ylabel('Read index')\n",
    "ax.set_title(f'gene symbol: {gs}    minMQ: {min_mapping_quality}    maxMQ: {max_mapping_quality}')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'./output/{gs}__long_mRNA__{suffix_str}.png', dpi=100)\n",
    "with open(f'./output/{gs}__junctions.tsv', 'w') as f:\n",
    "    f.write('JUNCTION_INDEX\\tPOSITION\\n')\n",
    "    for i, j in enumerate(filtered_junctions):\n",
    "        f.write(f'{i}\\t{contig}:{j}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have ipywidgets installed, you can make interactive plots by replacing\n",
    "# the following line with:\n",
    "#\n",
    "# %matplotlib widget\n",
    "#\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "coverage_transform = lambda x: x\n",
    "\n",
    "transformed_total_coverage_i = coverage_transform(total_coverage_i)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "nucleotide_to_color_map = {\n",
    "    'T': [1., 1., 0.],\n",
    "    'A': [0., 0., 1.],\n",
    "    'C': [1., 0., 0.],\n",
    "    'G': [0., 1., 0.]\n",
    "}\n",
    "gene_seq_im = np.asarray(list(map(nucleotide_to_color_map.get, gene_seq)))[None, :, :]\n",
    "ax.imshow(gene_seq_im, aspect='auto', extent=[fetch_start, fetch_end, 0., np.max(transformed_total_coverage_i)])\n",
    "ax.fill_between(np.arange(fetch_start, fetch_end + 1), transformed_total_coverage_i, color='white', alpha=0.7)\n",
    "ax.set_xlabel(f'Genomic position on {contig}')\n",
    "ax.set_title('Gene sequence')\n",
    "ax.set_ylim(bottom=0.)\n",
    "for i, junction in enumerate(filtered_junctions):\n",
    "    ax.axvline(junction, color='black', lw=1)\n",
    "    y_pos = 10 if i % 2 == 0 else 20\n",
    "    ax.text(junction, y_pos, str(i), **text_props)\n",
    "ax.set_ylabel('Coverage')\n",
    "\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-trouble",
   "metadata": {},
   "source": [
    "## (Optional) Manual isoform labeling\n",
    "\n",
    "Note: either run manual labeling or automatic labeling (coming up later)\n",
    "\n",
    "A decision tree such as,\n",
    "\n",
    "```\n",
    "isoform_decision_tree = {\n",
    "    (segment_index_1,): ('X', {\n",
    "        (segment_index_2, segment_index_3): ('A', {\n",
    "            (segment_index_4,): ('B', dict())\n",
    "        }),\n",
    "    }),\n",
    "    (segment_index_5,): ('Y', dict()),\n",
    "    (segment_index_6, segment_index_7): ('Z', dict())\n",
    "    }),\n",
    "}\n",
    "```\n",
    "\n",
    "is interpreted as follows:\n",
    "\n",
    "- if the read has coverage over `segment_index_1` and but not over both `segment_index_2` and `segment_index_3`, then decision for this branch is `X`\n",
    "  - else if it has coverage over both `segment_index_2` and `segment_index_3` but not over `segment_index_4`, then decision for this branch is `A`\n",
    "    - else if it has coverage over `segment_index_4`, then the deicison for this branch is `B`\n",
    "- if the read has coverage over `segment_index_5`, then the decision for this branch is `Y`\n",
    "- if the read has coverage over `segment_index_6` and `segment_index_7`, then enter decision `Z` (isoform suffix: Z)\n",
    "\n",
    "A read is parsed according to the decision tree and all compatible decision branches are identified. Ultimately, a unique name is generated by concatenating all compatible leaf nodes.\n",
    "\n",
    "More examples:\n",
    "\n",
    "```\n",
    "Segments covered     1  2  3  4  5  6  7\n",
    "Read_1               o  o  o  o  o  o  o   =>  Unassigned\n",
    "Read_2               X  o  o  o  o  o  o   =>  X\n",
    "Read_3               X  o  o  o  o  X  X   =>  XZ\n",
    "Read_4               X  o  o  o  o  o  X   =>  X\n",
    "Read_5               o  X  X  o  o  o  o   =>  Unassigned\n",
    "Read_6               o  o  o  o  X  o  o   =>  Y\n",
    "Read_7               X  o  o  X  X  o  o   =>  XY\n",
    "Read_8               o  o  o  o  x  X  X   =>  YZ\n",
    "Read_9               X  X  X  X  o  o  o   =>  B\n",
    "Read_10              X  X  X  o  X  X  X   =>  AYZ\n",
    "Read_11              X  X  o  X  X  X  X   =>  XYZ\n",
    "Read_12              X  X  X  X  X  X  o   =>  BY\n",
    "Read_13              X  X  X  X  X  X  X   =>  BYZ\n",
    "...\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Not all segments need to appear in the deicision tree;  the decision can be based on a handful of segments (e.g. variable exons)\n",
    "- The code extends a python tuple of segments to check for coverage at each node;  if testing for coverage over a single segment is intend, you must still enter it as a tuple, e.g. `(5,)` for segment index `5`\n",
    "- The decision tree is a nested dictionary;  To end the decision tree at some leaf node, use an empty dictionary, i.e. `dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def traverse_isoform_decision_tree(\n",
    "        segment_overlap_vector: np.ndarray,\n",
    "        decision_tree: dict,\n",
    "        coverage_threshold: float,\n",
    "        traversal=None) -> List[Tuple[List[Tuple[int]], dict]]:\n",
    "    \n",
    "    # root\n",
    "    if traversal == None:\n",
    "        traversal = [([], decision_tree)]\n",
    "        \n",
    "    extended = False\n",
    "    extended_traversal = []\n",
    "    for branch in traversal:\n",
    "        path = branch[0]\n",
    "        head_decision_tree = branch[1]\n",
    "        branch_extended = False\n",
    "        for segment_indices, (_, head_decision_subtree) in head_decision_tree.items():\n",
    "            if all(segment_overlap_vector[segment_index] >= coverage_threshold for segment_index in segment_indices):\n",
    "                branch_extended = True\n",
    "                extended = True\n",
    "                extended_traversal.append((path.copy() + [segment_indices], head_decision_subtree))\n",
    "        if not branch_extended and path != []:\n",
    "            extended_traversal.append(branch)\n",
    "    \n",
    "    if extended:\n",
    "        return traverse_isoform_decision_tree(\n",
    "            segment_overlap_vector,\n",
    "            decision_tree,\n",
    "            coverage_threshold,\n",
    "            extended_traversal)\n",
    "    else:\n",
    "        return traversal\n",
    "\n",
    "    \n",
    "def get_all_compat_isoforms(\n",
    "        segment_overlap_vector: np.ndarray,\n",
    "        decision_tree: dict,\n",
    "        coverage_threshold: float) -> List[List[int]]:\n",
    "    traversal = traverse_isoform_decision_tree(\n",
    "        segment_overlap_vector,\n",
    "        decision_tree,\n",
    "        coverage_threshold)\n",
    "    paths = []\n",
    "    for branch in traversal:\n",
    "        paths.append(branch[0])\n",
    "    return paths\n",
    "\n",
    "\n",
    "def generate_isoform_name(\n",
    "        paths: List[List[int]],\n",
    "        decision_tree: dict,\n",
    "        naming_prefix: str) -> str:\n",
    "    suffix_set = set()\n",
    "    for path in paths:\n",
    "        subtree = decision_tree\n",
    "        suffix = 'Unassigned'\n",
    "        while len(path):\n",
    "            suffix = subtree[path[0]][0]\n",
    "            subtree = subtree[path[0]][1]\n",
    "            path = path[1:]\n",
    "        suffix_set.add(suffix)\n",
    "    return naming_prefix + '_' + ''.join(sorted(list(suffix_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-insurance",
   "metadata": {},
   "source": [
    "### Decision tree defined by vectorization intervals\n",
    "\n",
    "Use this cell if you know the vectorization interval index you would like to use to build the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage threshold\n",
    "coverage_threshold = 0.1\n",
    "naming_prefix = gs\n",
    "\n",
    "# mininum number of reads for an isoform label;  if lower, it will be demoted to Unassigned\n",
    "min_reads_per_cluster = 10\n",
    "\n",
    "isoform_decision_tree = {\n",
    "  (1,): ('A', dict()),\n",
    "  (3,): ('B', dict()),\n",
    "  (4,): ('C', dict()),\n",
    "  (14,): ('D', dict()),\n",
    "  (28,): ('E', dict()),\n",
    "  (28,): ('F', dict())\n",
    "}\n",
    "\n",
    "\n",
    "# print interval list appearing in the decision tree\n",
    "def print_decision_tree(decision_tree):\n",
    "    for interval_indices, (name, sub_tree) in decision_tree.items():\n",
    "        for interval_index in interval_indices:\n",
    "            print(f'Vectorizatrion interval {interval_index} coordinates (used in {name}): {vectorization_interval_list[interval_index]}')\n",
    "        print_decision_tree(sub_tree)\n",
    "        \n",
    "print_decision_tree(isoform_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-square",
   "metadata": {},
   "source": [
    "### Decision tree defined by genomic intervals\n",
    "\n",
    "Use this cell if you know the genomic position of exons you would like to use to build the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage threshold\n",
    "coverage_threshold = 0.1\n",
    "naming_prefix = gs\n",
    "\n",
    "# mininum number of reads for an isoform label;  if lower, it will be demoted to Unassigned\n",
    "min_reads_per_cluster = 10\n",
    "\n",
    "from isoform_utils import Interval, get_overlap_fraction\n",
    "\n",
    "def get_overlapping_vectorization_intervals(interval: Interval, overlap_threshold=0.75) -> Tuple[int]:\n",
    "    vect_indices = []\n",
    "    for vect_index, vect_interval in enumerate(vectorization_interval_list):\n",
    "        if get_overlap_fraction(vect_interval.start, vect_interval.end, interval.start, interval.end) > overlap_threshold:\n",
    "            vect_indices.append(vect_index)\n",
    "    return tuple(vect_indices)\n",
    "    \n",
    "interval_Z = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198692373, end=198692959)\n",
    "interval_W = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198692959, end=198693202)\n",
    "interval_O = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198703297, end=198703372)\n",
    "interval_A = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198696711, end=198696909)\n",
    "interval_B = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198699563, end=198699704)\n",
    "interval_C = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198702386, end=198702530)\n",
    "interval_L = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198732300, end=198732390)\n",
    "interval_XL = Interval(feature_type='vectorization_interval_ENSG00000081237.20', contig='chr1', start=198741868, end=198742026)\n",
    "\n",
    "# PTPRC assignments\n",
    "isoform_decision_tree = {\n",
    "    get_overlapping_vectorization_intervals(interval_O): ('O', {\n",
    "        get_overlapping_vectorization_intervals(interval_A): ('A', dict()),\n",
    "        get_overlapping_vectorization_intervals(interval_B): ('B', dict()),\n",
    "        get_overlapping_vectorization_intervals(interval_C): ('C', dict())\n",
    "    }),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "isoform_names_m = []\n",
    "for segment_overlap_vector in filtered_vectorized_alignments_mv:\n",
    "    isoform_name = generate_isoform_name(\n",
    "        get_all_compat_isoforms(\n",
    "            segment_overlap_vector,\n",
    "            isoform_decision_tree,\n",
    "            coverage_threshold),\n",
    "        isoform_decision_tree,\n",
    "        naming_prefix)\n",
    "    isoform_names_m.append(isoform_name)\n",
    "\n",
    "unassigned_name = naming_prefix + '_Unassigned'\n",
    "\n",
    "# filter\n",
    "from collections import Counter\n",
    "\n",
    "isoform_names_counter = Counter(isoform_names_m)\n",
    "isoform_name_reassignment_map = dict()\n",
    "for isoform_name, counts in isoform_names_counter.items():\n",
    "    if counts >= min_reads_per_cluster:\n",
    "        isoform_name_reassignment_map[isoform_name] = isoform_name\n",
    "    else:\n",
    "        isoform_name_reassignment_map[isoform_name] = unassigned_name\n",
    "isoform_names_m = list(map(isoform_name_reassignment_map.get, isoform_names_m))\n",
    "                       \n",
    "# post\n",
    "isoform_names_set = set(isoform_names_m)\n",
    "try:\n",
    "    isoform_names_set.remove(unassigned_name)\n",
    "except KeyError:\n",
    "    pass\n",
    "isoform_name_to_isoform_label_map = {unassigned_name: -1}\n",
    "for i_isoform, isoform_name in enumerate(sorted(isoform_names_set)):\n",
    "    isoform_name_to_isoform_label_map[isoform_name] = i_isoform\n",
    "isoform_label_to_isoform_name_map = {label: name for name, label in isoform_name_to_isoform_label_map.items()}\n",
    "isoform_labels_m = np.asarray(list(map(isoform_name_to_isoform_label_map.get, isoform_names_m)))\n",
    "isoform_labels_set = set(isoform_labels_m)\n",
    "isoform_labels_list = sorted(isoform_labels_set)\n",
    "isoform_names_list = list(map(isoform_label_to_isoform_name_map.get, isoform_labels_list))\n",
    "num_isoform_clusters = len(isoform_labels_set)\n",
    "\n",
    "print(f'Number of isoform clusters: {num_isoform_clusters}')\n",
    "print(isoform_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-pixel",
   "metadata": {},
   "source": [
    "## (Optional) Automatic isoform labeling\n",
    "\n",
    "Using DBSCAN clustering.\n",
    "\n",
    "Note: either run manual labeling (ealier) or automatic labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# clustering resolution\n",
    "naming_prefix = gs\n",
    "eps = 1.0\n",
    "min_samples = 5\n",
    "\n",
    "# what features to use for clustering?\n",
    "clustering_input_data_mv = filtered_vectorized_alignments_mv\n",
    "\n",
    "# DBSCAN\n",
    "isoform_labels_m = DBSCAN(\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    metric='l1').fit_predict(clustering_input_data_mv)\n",
    "\n",
    "isoform_labels_set = set(isoform_labels_m)\n",
    "isoform_labels_list = sorted(isoform_labels_set)\n",
    "unassigned_name = naming_prefix + '_Unassigned'\n",
    "isoform_label_to_isoform_name_map = dict()\n",
    "for i_isoform, isoform_label in enumerate(isoform_labels_list):\n",
    "    isoform_label_to_isoform_name_map[isoform_label] = f'{naming_prefix}_{str(isoform_label)}'\n",
    "isoform_label_to_isoform_name_map[-1] = unassigned_name\n",
    "isoform_name_to_isoform_label_map = {name: label for label, name in isoform_label_to_isoform_name_map.items()}\n",
    "isoform_names_list = list(map(isoform_label_to_isoform_name_map.get, isoform_labels_list))\n",
    "isoform_names_set = set(isoform_names_list)\n",
    "num_isoform_clusters = len(isoform_labels_set)\n",
    "\n",
    "print(f'Number of isoform clusters: {num_isoform_clusters}')\n",
    "print(isoform_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-bhutan",
   "metadata": {},
   "source": [
    "## Plot coverage of assigned transcripts to each isoform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "vertical_scale = 1.5\n",
    "min_covearge_for_compressed_plot = 5\n",
    "\n",
    "# mark positions with above-threshold coverage for plotting\n",
    "filtered_total_coverage_i = np.asarray(filtered_coverage_matrix_mi.sum(0)).flatten()\n",
    "plot_positions_set = set()\n",
    "for pos in np.nonzero(filtered_total_coverage_i >= min_covearge_for_compressed_plot)[0].tolist() + filtered_junctions:\n",
    "    plot_positions_set.update(set(\n",
    "        np.arange(pos - nonzero_coverage_positions_pad, pos + nonzero_coverage_positions_pad, + 1).tolist()))\n",
    "plot_positions = sorted(set(pos for pos in plot_positions_set if 0 <= pos < fetch_end - fetch_start))\n",
    "\n",
    "# calculate coverage for each isoform\n",
    "normalized_isoform_covearge_i_list = []\n",
    "for i_isoform in range(num_isoform_clusters):\n",
    "    isoform_label = isoform_labels_list[i_isoform]\n",
    "    isoform_coverage_i = np.asarray(filtered_coverage_matrix_mi[isoform_labels_m == isoform_label].sum(0)).flatten()\n",
    "    isoform_coverage_i = isoform_coverage_i / (1e-6 + np.sum(isoform_coverage_i))\n",
    "    normalized_isoform_covearge_i_list.append(isoform_coverage_i)\n",
    "\n",
    "# generate annotations for plooting\n",
    "feature_to_annotation_track_map = iso.get_feature_to_annotation_track_map(\n",
    "    feature_interval_list=feature_interval_list,\n",
    "    included_feature_types=included_feature_types,\n",
    "    ref_contig=contig,\n",
    "    ref_interval_start=fetch_start,\n",
    "    ref_interval_end=fetch_end)\n",
    "\n",
    "# feature annotations\n",
    "feature_to_annotation_track_map = iso.get_feature_to_annotation_track_map(\n",
    "    feature_interval_list=feature_interval_list,\n",
    "    included_feature_types=included_feature_types,\n",
    "    ref_contig=contig,\n",
    "    ref_interval_start=fetch_start,\n",
    "    ref_interval_end=fetch_end)\n",
    "\n",
    "# add junctions to annotations\n",
    "junctions_track_i = np.zeros((fetch_end - fetch_start + 1),)\n",
    "trunc_junctions = [junction for junction in filtered_junctions if fetch_end > junction >= fetch_start]\n",
    "junctions_track_i[np.asarray(trunc_junctions) - fetch_start] = 1.\n",
    "feature_to_annotation_track_map['junction'] = junctions_track_i\n",
    "\n",
    "\n",
    "nrows = num_isoform_clusters\n",
    "fig, axs = plt.subplots(ncols=1, nrows=nrows, figsize=(30, nrows * vertical_scale))\n",
    "    \n",
    "for i_isoform in range(num_isoform_clusters):\n",
    "    \n",
    "    isoform_label = isoform_labels_list[i_isoform]    \n",
    "    isoform_name = isoform_names_list[i_isoform]    \n",
    "    num_reads_support = np.sum(isoform_labels_m == isoform_label)\n",
    "    normalized_isoform_covearge_i = normalized_isoform_covearge_i_list[i_isoform]\n",
    "    nnz_normalized_isoform_covearge_i = normalized_isoform_covearge_i[plot_positions]\n",
    "    ax = axs[i_isoform]\n",
    "    \n",
    "    # annotate axis\n",
    "    vis.annotate_axis_with_tracks(\n",
    "        feature_to_annotation_track_map,\n",
    "        included_features=[\n",
    "            'exon',\n",
    "            'junction'],\n",
    "        feature_to_color_map={\n",
    "            'exon': 'blue',\n",
    "            'junction': 'white',\n",
    "            'intron': 'yellow'},\n",
    "        ax=ax,\n",
    "        included_position_mask=plot_positions,\n",
    "        alpha_region=0.2)\n",
    "\n",
    "    # plot coverage\n",
    "    ax.fill_between(\n",
    "        np.arange(len(nnz_normalized_isoform_covearge_i)), nnz_normalized_isoform_covearge_i,\n",
    "        color='white'\n",
    "    )\n",
    "            \n",
    "    if i_isoform != num_isoform_clusters - 1:\n",
    "        ax.set_xticks([])\n",
    "    \n",
    "    ax.set_xlim((-1000, len(plot_positions) + 100))\n",
    "    ax.set_ylim(bottom=0.)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim((1e-6, 1e-0))\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.text(-1000, 1e-5, f'{isoform_name}: {num_reads_support}', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-separation",
   "metadata": {},
   "source": [
    "## Visualize where each isoform is expressed in the short reads embedding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "input_prefix = 'M132TS_immune.revised_v2.harmonized'\n",
    "output_path = 'output/t-cell-vdj-cite-seq'\n",
    "final_short_adata_raw_h5_path = os.path.join(repo_root, output_path, f'{input_prefix}.short.stringtie.final.raw.h5ad')\n",
    "final_short_adata_sct_h5_path = os.path.join(repo_root, output_path, f'{input_prefix}.short.stringtie.final.sct.h5ad')\n",
    "final_long_adata_raw_h5_path = os.path.join(repo_root, output_path, f'{input_prefix}.long.stringtie.final.raw.h5ad')\n",
    "final_long_adata_sct_h5_path = os.path.join(repo_root, output_path, f'{input_prefix}.long.stringtie.final.sct.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GEX adata\n",
    "# note: we are loading the output of mb_07_harmonize_long_short\n",
    "leiden_key = 'mehrtash_leiden'\n",
    "embedding_key = 'X_umap_SCT_short'\n",
    "\n",
    "adata = sc.read(final_short_adata_raw_h5_path)\n",
    "\n",
    "# color by leiden cluster\n",
    "filtered_alignment_barcode_list = [\n",
    "    alignment.get_tag('CB') for alignment in filtered_alignment_list]\n",
    "\n",
    "# map from barcode to leiden\n",
    "barcode_to_leiden_id_map = dict()\n",
    "adata_barcode_list = adata.obs.index.values\n",
    "adata_leiden_id_list = np.asarray(adata.obs[leiden_key].values)\n",
    "for barcode, leiden_id in zip(adata_barcode_list, adata_leiden_id_list):\n",
    "    barcode_to_leiden_id_map[barcode] = leiden_id\n",
    "\n",
    "# leiden id for each alignment\n",
    "filtered_alignment_leiden_list = []\n",
    "for barcode in filtered_alignment_barcode_list:\n",
    "    leiden_id = '-1'\n",
    "    if barcode in barcode_to_leiden_id_map:\n",
    "        leiden_id = barcode_to_leiden_id_map[barcode]\n",
    "    filtered_alignment_leiden_list.append(leiden_id)\n",
    "\n",
    "# all leiden ids\n",
    "adata_leiden_id_list = adata.obs[leiden_key].values.categories.values.tolist()\n",
    "\n",
    "# colors\n",
    "alpha = 0.8\n",
    "\n",
    "import colorcet as cc\n",
    "leiden_colors = [cc.cm.glasbey(i)[:3] + (alpha,) for i in range(len(adata_leiden_id_list))]\n",
    "adata_color_list = []\n",
    "leiden_id_to_idx_map = {leiden_id: idx for idx, leiden_id in enumerate(adata_leiden_id_list)}\n",
    "for leiden_id in adata.obs[leiden_key]:\n",
    "    if leiden_id == '-1':\n",
    "        color = (0., 0., 0., 0.,)\n",
    "    else:\n",
    "        color = leiden_colors[leiden_id_to_idx_map[leiden_id]]\n",
    "    adata_color_list.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of cells per isoform (otherwise, isoform not show)\n",
    "min_cells_per_isoform = 1\n",
    "\n",
    "# leiden\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(\n",
    "    adata.obsm[embedding_key][:, 0],\n",
    "    adata.obsm[embedding_key][:, 1],\n",
    "    color=adata_color_list,\n",
    "    s=5)\n",
    "\n",
    "for leiden_id in adata_leiden_id_list:\n",
    "    x_values = adata.obsm[embedding_key][adata.obs[leiden_key] == leiden_id, 0]\n",
    "    y_values = adata.obsm[embedding_key][adata.obs[leiden_key] == leiden_id, 1]\n",
    "    x_c, y_c = np.mean(x_values), np.mean(y_values)\n",
    "    ax.text(x_c, y_c, leiden_id, bbox=dict(boxstyle='round', facecolor='black', alpha=0.5))\n",
    "\n",
    "# highlights\n",
    "ncols = 8\n",
    "nrows = int(np.ceil(len(set(isoform_labels_set)) / ncols))\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3 * ncols, 3 * nrows))\n",
    "\n",
    "barcode_to_adata_idx_map = {adata.obs.index.values[idx]: idx for idx in range(len(adata))}\n",
    "\n",
    "i_ax = 0\n",
    "for i_isoform in range(num_isoform_clusters):\n",
    "    isoform_id_highlight = isoform_labels_list[i_isoform]\n",
    "    isoform_name = isoform_names_list[i_isoform]   \n",
    "    ax = axs.flatten()[i_ax]\n",
    "    \n",
    "    highlight_barcodes = [\n",
    "        barcode for i, barcode in enumerate(filtered_alignment_barcode_list)\n",
    "        if isoform_labels_m[i] == isoform_id_highlight]\n",
    "    highlight_adata_indices = []\n",
    "    highlight_leiden_ids = []\n",
    "    for barcode in highlight_barcodes:\n",
    "        if barcode in barcode_to_adata_idx_map:\n",
    "            highlight_adata_indices.append(barcode_to_adata_idx_map[barcode])\n",
    "    \n",
    "    n_expressing_cells = len(highlight_adata_indices)\n",
    "    if n_expressing_cells >= min_cells_per_isoform:\n",
    "        \n",
    "        ax.scatter(\n",
    "            adata.obsm[embedding_key][:, 0],\n",
    "            adata.obsm[embedding_key][:, 1],\n",
    "            color='gray',\n",
    "            s=10,\n",
    "            alpha=0.05)\n",
    "\n",
    "        ax.scatter(\n",
    "            adata.obsm[embedding_key][highlight_adata_indices, 0],\n",
    "            adata.obsm[embedding_key][highlight_adata_indices, 1],\n",
    "            c='white',\n",
    "            s=5, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'{isoform_name}')\n",
    "        i_ax += 1\n",
    "    \n",
    "    else:        \n",
    "        log_info(f'Isoform {isoform_name} has only {n_expressing_cells} expressing cells and was not shown.')\n",
    "    \n",
    "# get rid of extra panels\n",
    "for j in range(i_ax, len(axs.flatten())):\n",
    "    axs.flatten()[j].axis('off')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-soldier",
   "metadata": {},
   "source": [
    "## Generate query name list that map the selected gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "alignment_list_by_isoform_map = defaultdict(list)\n",
    "alignment_list_by_leiden_id_map = defaultdict(list)\n",
    "prefix = 'M132TS'\n",
    "\n",
    "for isoform_name, alignment in zip(isoform_names_m, filtered_alignment_list):\n",
    "    alignment_list_by_isoform_map[isoform_name].append(alignment)\n",
    "    \n",
    "for barcode, alignment in zip(filtered_alignment_barcode_list, filtered_alignment_list):\n",
    "    try:\n",
    "        leiden_id = barcode_to_leiden_id_map[barcode]\n",
    "        alignment_list_by_leiden_id_map[leiden_id].append(alignment)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# export TXT\n",
    "for isoform_name, alignment_list in alignment_list_by_isoform_map.items():\n",
    "    with open(os.path.join(repo_root, f'output/t-cell-vdj-cite-seq/{prefix}__{isoform_name}__query_names.txt'), 'w') as f:\n",
    "        for alignment in alignment_list:\n",
    "            f.write(alignment.qname + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-hopkins",
   "metadata": {},
   "source": [
    "## Export a `.bam` of all alignments\n",
    "\n",
    "- all queries\n",
    "- queries stratified by leiden cluster index\n",
    "\n",
    "Note: `ggsashimi` does not accept CIGAR strings that include `X` and `=`.  We will replace these operators with `M` (match) in the produced `.bam` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "_CIGAR_SIMPLIFY_MAP = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 5,\n",
    "    6: 6,\n",
    "    7: 0,\n",
    "    8: 0,\n",
    "    9: 9\n",
    "}\n",
    "\n",
    "def simplify_cigartuples(cigartuples):\n",
    "    \"\"\"\n",
    "        The operations are:\n",
    "        +-----+--------------+-----+\n",
    "        |M    |BAM_CMATCH    |0    |\n",
    "        +-----+--------------+-----+\n",
    "        |I    |BAM_CINS      |1    |\n",
    "        +-----+--------------+-----+\n",
    "        |D    |BAM_CDEL      |2    |\n",
    "        +-----+--------------+-----+\n",
    "        |N    |BAM_CREF_SKIP |3    |\n",
    "        +-----+--------------+-----+\n",
    "        |S    |BAM_CSOFT_CLIP|4    |\n",
    "        +-----+--------------+-----+\n",
    "        |H    |BAM_CHARD_CLIP|5    |\n",
    "        +-----+--------------+-----+\n",
    "        |P    |BAM_CPAD      |6    |\n",
    "        +-----+--------------+-----+\n",
    "        |=    |BAM_CEQUAL    |7    |\n",
    "        +-----+--------------+-----+\n",
    "        |X    |BAM_CDIFF     |8    |\n",
    "        +-----+--------------+-----+\n",
    "        |B    |BAM_CBACK     |9    |\n",
    "        +-----+--------------+-----+\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace X and = with M\n",
    "    new_cigartuples = []\n",
    "    for k, v in cigartuples:\n",
    "        new_cigartuples.append((_CIGAR_SIMPLIFY_MAP[k], v))\n",
    "    \n",
    "    # merge tandem maches\n",
    "    simplified_cigartuples = []\n",
    "    for g in groupby(new_cigartuples, key=lambda cigartuple: cigartuple[0]):\n",
    "        cigar_op = g[0]\n",
    "        tups = list(g[1])\n",
    "        if cigar_op == 0:  # match\n",
    "            total_matches = sum(tup[1] for tup in tups)\n",
    "            simplified_cigartuples.append((0, total_matches))\n",
    "        else:\n",
    "            simplified_cigartuples += tups\n",
    "    \n",
    "    return simplified_cigartuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export BAM (all queries)\n",
    "header = pysam.AlignmentFile(bam_path).header\n",
    "with pysam.AlignmentFile(f'./output/bam/{prefix}.{gs}.bam', mode='wb', header=header) as f:\n",
    "    for isoform_name, alignment_list in alignment_list_by_isoform_map.items():\n",
    "        for alignment in alignment_list:\n",
    "            new_alignment = pysam.AlignedSegment.fromstring(alignment.to_string(), header)\n",
    "            new_alignment.cigartuples = simplify_cigartuples(new_alignment.cigartuples)\n",
    "            new_alignment.set_tag('IN', isoform_name)\n",
    "            f.write(new_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export BAM (stratified by leiden id)\n",
    "header = pysam.AlignmentFile(bam_path).header\n",
    "for leiden_id in adata_leiden_id_list:\n",
    "    fixed_leiden_id = leiden_id.replace(' ', '_').replace('/', '_')\n",
    "    with pysam.AlignmentFile(f'./output/bam/{prefix}.{gs}.{fixed_leiden_id}.bam', mode='wb', header=header) as f:\n",
    "        for alignment in alignment_list_by_leiden_id_map[leiden_id]:\n",
    "            new_alignment = pysam.AlignedSegment.fromstring(alignment.to_string(), header)\n",
    "            new_alignment.cigartuples = simplify_cigartuples(new_alignment.cigartuples)\n",
    "            new_alignment.set_tag('IN', isoform_name)\n",
    "            f.write(new_alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-projector",
   "metadata": {},
   "source": [
    "## Export an AnnData object for the manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "adata_barcodes = adata.obs.index.values\n",
    "expr_ni = np.zeros((len(adata_barcodes), len(isoform_names_list)))\n",
    "\n",
    "barcode_to_adata_row_idx = {barcode: row_idx for row_idx, barcode in enumerate(adata_barcodes)}\n",
    "isoform_name_to_adata_col_idx = {isoform_name: col_idx for col_idx, isoform_name in enumerate(isoform_names_list)}\n",
    "for isoform_name, barcode in zip(isoform_names_m, filtered_alignment_barcode_list):\n",
    "    if barcode in barcode_to_adata_row_idx:\n",
    "        row_idx = barcode_to_adata_row_idx[barcode]\n",
    "        col_idx = isoform_name_to_adata_col_idx[isoform_name]\n",
    "        expr_ni[row_idx, col_idx] += 1\n",
    "\n",
    "prefix = 'manual__'\n",
    "adata_isoform_names_list = [prefix + isoform_name for isoform_name in isoform_names_list]\n",
    "new_var = pd.DataFrame(\n",
    "    dict(\n",
    "        transcript_ids=adata_isoform_names_list,\n",
    "        gene_ids=[prefix + gid] * len(adata_isoform_names_list), \n",
    "        gene_names=[prefix + gs] * len(adata_isoform_names_list), \n",
    "        transcript_names=adata_isoform_names_list,\n",
    "        de_novo_gene_ids=['N/A'] * len(adata_isoform_names_list),\n",
    "        de_novo_transcript_ids=adata_isoform_names_list,\n",
    "        is_de_novo=[True] * len(adata_isoform_names_list),\n",
    "        is_gene_id_ambiguous=[False] * len(adata_isoform_names_list)),\n",
    "    index=adata_isoform_names_list)\n",
    "\n",
    "new_adata = sc.AnnData(\n",
    "    X=csr_matrix(expr_ni),\n",
    "    var=new_var)\n",
    "\n",
    "new_adata.obs.index = adata.obs.index\n",
    "new_adata.write(os.path.join(repo_root, 'output/t-cell-vdj-cite-seq/manual_annotations', f'adata_long_manual__{gs}.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-effects",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
